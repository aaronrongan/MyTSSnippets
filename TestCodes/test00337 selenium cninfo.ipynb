{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#目的:\n",
    "* 190925   用selenium打开，然后抓取网页中的信息，如财报地址\n",
    "* 191204    需要安装geckodriver\n",
    "* 191211    实现自动翻页，生成一个dataframe，导出为csv文件\n",
    "* 191212    对自选股中的进行筛选，用短信提示\n",
    "* 191213    \n",
    "* 200116    要实现在年份和季度栏自动输入，这样才可以提取需要的时间数据\n",
    "\n",
    "#要点   \n",
    "* 1. 2次间隔sleep\n",
    "* 2. 解码中文网页\n",
    "    ht = driver.page_source\n",
    "    ht=ht.encode('gb18030').decode('gb18030')\n",
    "* 3. 内容解析用BeautifulSoup实现，用到find/find all以及html语言中的td/table等元素\n",
    "* 4. 网页翻页实现\n",
    "* 5. 创建一个DataFrame，每读一行建一个dictionary，df.df=df.append(dict1,ignore_index=True)\n",
    "* 6. 保存为CSV时，编码要用utf-8-sig    df.to_csv('c:/1.csv', encoding='utf-8-sig')\n",
    "* 7. dataframes的批量筛选 df=df[df['name'].isin(needrow)]\n",
    "    如果是用~，比如df=df[~df['name'].。。。表示不在需要的行里\n",
    "* 8. 将dataframe的行转为strings \n",
    "*      for index,row in conbondsdf.iterrows():\n",
    "*          mailsubject=row['证券简称']\n",
    "*          mailcontent=','.join(str(i) for i in row)\n",
    "* 9.  driver.find_elements_by_class_name('stock-select')[1] 找一个element\n",
    "*       a = driver.find_elements_by_class_name('stock-select')[1]\n",
    "* 10. from selenium.webdriver.support.ui import Select \n",
    "*       select = Select(driver.find_elements_by_class_name('stock-select')[3])\n",
    "*       select.select_by_index(3)\n",
    "* 11. ele.location 知道坐标 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import unittest\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib3\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import socket\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from selenium.webdriver.support.ui import Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#当前文件的路径\n",
    "pwd = os.getcwd()\n",
    "#当前文件的父路径\n",
    "father_path=os.path.abspath(os.path.dirname(pwd)+os.path.sep+\".\")\n",
    "\n",
    "downloadpath=father_path + \"/DataRepository/db00111/\"\n",
    "downloadfilename='2020Q1.xlsx'\n",
    "\n",
    "selectedcodepath=father_path + \"/DataRepository/db00021/stocklist1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profileDir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b0f903e6fa1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFirefoxProfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofileDir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFirefox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'profileDir' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# browser = webdriver.Chrome()\n",
    "# browser.get('https://www.taobao.com/')\n",
    "# wait = WebDriverWait(browser,10)#指定最长等待时间\n",
    "#传入等待条件presence_of_element_located，代表节点出现，参数是节点的定位元组ID为q的搜索框#\n",
    "# 10秒内如果ID为q的节点成功加载出来就返回该节点\n",
    "# input = wait.until(EC.presence_of_element_located((By.ID,'q')))\n",
    "# #对于按钮，则将等待条件更改为element_to_be_clickable表示可点击，如果10秒内可点击就返回这个按钮节点\n",
    "# button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR,'.btn-search')))\n",
    "# print(input,button)\n",
    "\n",
    "\n",
    "# import TimeUnit\n",
    "\n",
    "# class Register(unittest.TestCase):\n",
    "# \tdef setUp(self):\n",
    "# \t\tself.profileDir = \"C:/Users/Administrator/AppData/Roaming/Mozilla/Firefox/Profiles/xngshdad.Python\"\n",
    "# \t\tself.profile = webdriver.FirefoxProfile(self.profileDir)\n",
    "# \t\tself.driver = webdriver.Firefox(self.profile)\n",
    "# \t\tself.driver.implicitly_wait(30)\n",
    "# \t\tself.base_url = \"www.baidu.com\"\n",
    "# \t\tself.verificationErrors = []\n",
    "\n",
    "# profileDir = \"C:/Users/Administrator/AppData/Roaming/Mozilla/Firefox/Profiles/3n0dd184.default-release\"\n",
    "# \n",
    "\n",
    "hostname=socket.gethostname()\n",
    "if hostname=='MyWorkstation':\n",
    "    profileDir = \"C:/Users/Aaron/AppData/Roaming/Mozilla/Firefox/Profiles/yn80ouvt.default\" #用于MYWorkStation\n",
    "elif hostname=='MyMacWin10':\n",
    "    profileDir = \"C:/Users/Aaron/AppData/Roaming/Mozilla/Firefox/Profiles/ewg4x7y5.default-release\" #用于MyMacwin\n",
    "elif hostname==\"DESKTOP-DEFM935\":\n",
    "    a=1\n",
    "    \n",
    "\n",
    "profile =webdriver.FirefoxProfile(profileDir)\n",
    "driver = webdriver.Firefox(profile)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_url = \"www.baidu.com\"cls\n",
    "# driver=webdriver.Firefox()\n",
    "\n",
    "# driver.get('https://www.lixinger.com/analytics/company/sz/300383/detail/announcement?type=all&page-index=0/') #所有的公告\n",
    "# driver.get('https://www.lixinger.com/analytics/company/sz/000001/detail/announcement?type=fs&page-index=0') #财报\n",
    "# driver.get('http://webapi.cninfo.com.cn/#/thematicStatistics')\n",
    "\n",
    "# driver.get('http://webapi.cninfo.com.cn/api/sysapi/p_sysapi1024?tdate=2019-12-10')\n",
    "\n",
    "driver.get('http://webapi.cninfo.com.cn/#/thematicStatistics?id=538')\n",
    "\n",
    "#业绩大幅上升链接, 但是显示未授权\n",
    "# driver.get('http://webapi.cninfo.com.cn/api/sysapi/p_sysapi1066?apiname=p_sysapi1040')\n",
    "\n",
    "# buyers=driver.find_elements_by_xpath('//div[@title=“”]')\n",
    "# elements=driver.find_elements_by_xpath('//div[1]/div/div/div[3]')\n",
    "\n",
    "# elements=driver.find_elements_by_xpath('//div[@class=\"table-container\"]')\n",
    "\n",
    "# print(elements[0].getdis)\n",
    "\n",
    "# ht = driver.page_source\n",
    "# ht = driver.page_source\n",
    "# print(ht)\n",
    "# print(elements[0]])\n",
    "\n",
    "# soup = BeautifulSoup(ht,'html.parser')\n",
    "# WebDriverWait(driver,30)\n",
    "\n",
    "# \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ht = driver.page_source\n",
    "ht=ht.encode('gb18030').decode('gb18030')\n",
    "# ht=ht.encode(\"raw_unicode_escape\")\n",
    "\n",
    "#print(ht)\n",
    "# print(ht.)\n",
    "\n",
    "# print(ht.encode('utf-8'))\n",
    "# print(ht.encode('gb18030').encode(\"raw_unicode_escape\").decode(\"utf-8\"))\n",
    "\n",
    "soup =BeautifulSoup(ht, 'lxml')\n",
    "# print(soup)\n",
    "# a = driver.find_element_by_link_text('下一页')\n",
    "# a.click()\n",
    "\n",
    "#################################\n",
    "# CompanyAddress = soup.find('table', id=\"contentTable\").find_all('a')\n",
    "# for each in CompanyAddress:\n",
    "#     print(each.get('href'))\n",
    "################################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 如何键入需要的年份和季度？\n",
    "a = driver.find_element_by_id('seee1_sele').send_keys(Keys.BACKSPACE)\n",
    "a = driver.find_element_by_id('seee1_sele').send_keys('2019') #(Keys.UP)\n",
    "time.sleep(1)\n",
    "driver.find_element_by_id('seee1_sele').send_keys(Keys.ENTER)\n",
    "\n",
    "\n",
    "\n",
    "# select = Select(driver.find_elements_by_class_name('stock-select')[0])\n",
    "\n",
    "# select.select_by_index(0)\n",
    "# select.select_by_index(1)\n",
    "# select.select_by_value(\"2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select类操作\n",
    "\n",
    "\n",
    "# select = Select(driver.find_element_by_css_selector(\"[class='filter-condition condition2']\"))\n",
    "\n",
    "select = Select(driver.find_elements_by_class_name('stock-select')[3])\n",
    "\n",
    "select.select_by_index(3)\n",
    "\n",
    "\n",
    "# a = driver.find_element_by_class_name(\"filter-condition condition1\") #.send_keys('2019') #(Keys.UP)\n",
    "\n",
    "# a = driver.find_elements_by_class_name('stock-select')[1]\n",
    "# a.click()\n",
    "\n",
    "############################\n",
    "# element =driver.find_element_by_css_selector(\"[class='filter-condition condition2']\")#.send_keys('年报')\n",
    "# a.click()\n",
    "# ActionChains(driver).move_to_element(element).perform()\n",
    "\n",
    "# a.click()\n",
    "\n",
    "# driver.find_element_by_id('seee1_sele').send_keys(Keys.ENTER)\n",
    "# a.click()\n",
    "\n",
    "# a = driver.find_element_by_class_name('stock-select').send_keys('年报')\n",
    "# a.click()\n",
    "\n",
    "# a = driver.find_element_by_xpath('/html/body/div/div/div/div/div/div/div/div/div/div/div/div/div/div/div/div/select')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': 239, 'x': 1009}\n"
     ]
    }
   ],
   "source": [
    "#曲线救国，先定位旁边的元素，然后点击\n",
    "b =driver.find_elements_by_class_name('stock-select')[3]\n",
    "print(b.location)\n",
    "b.is_displayed()\n",
    "\n",
    "action = ActionChains(driver)\n",
    "action.move_to_element(b)\n",
    "action.move_by_offset(70,0)\n",
    "action.click()\n",
    "action.perform()\n",
    "# ActionChains(driver).move_by_offset(100, 0).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#点击搜索\n",
    "\n",
    "# ele.submit()\n",
    "# ele.click()\n",
    "\n",
    "# action = ActionChains(driver)\n",
    "#\n",
    "#################这里的元素无法点击，position为0，所以无法点击\n",
    "# ele=driver.find_element_by_css_selector(\"[class='stock-search btn thematicStatisticsBtn btn-primary']\")\n",
    "# print(ele.location)\n",
    "# ele.is_displayed()\n",
    "\n",
    "# ActionChains(driver).move_to_element(ele)\n",
    "\n",
    "# ActionChains(driver).move_by_offset(200, 100).context_click()\n",
    "\n",
    "# ele.click()\n",
    "# action.move_to_element(ele)\n",
    "# action.move_by_offset(5,-5)\n",
    "# action.click()\n",
    "\n",
    "# action.send_keys(Keys.F1)\n",
    "# action.perform()\n",
    "\n",
    "# action.move_to_element(ele).move_by_offset(5,5).click()\n",
    "# action.perform()\n",
    "# action.move_to_element(ele).perform()\n",
    "        \n",
    "# element.click()\n",
    "# driver.find_element_by_id('seee1_sele').send_keys(Keys.ENTER)\n",
    "\n",
    "\n",
    "# js = 'document.getElementById(\"su\").click();'\n",
    "\n",
    "# driver.execute_script(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ele=driver.find_element_by_css_selector(\"[class='stock-search btn thematicStatisticsBtn btn-primary']\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###建立一个表格头，抓取元素对应的表头字段\n",
    "\n",
    "##########找到表格头\n",
    "thead=soup.find('thead').find_all('th')#.find(\"div\").find_all('title')\n",
    "# print(thead)\n",
    "headcolumnlist=[]\n",
    "for each in thead:\n",
    "    headcolumnlist.append(each.find(\"div\").text)\n",
    "# print(headcolumnlist)\n",
    "print('\\n')\n",
    "columnnumbers=len(headcolumnlist)\n",
    "# headcolumnlist[0]\n",
    "\n",
    "\n",
    "############找到一共多少页\n",
    "pagelast=driver.find_element_by_xpath(\"//li[@class='page-last']/a\")\n",
    "totalpages=int(pagelast.text)\n",
    "# totalpages\n",
    "\n",
    "\n",
    "#创建一个空DataFrame，然后插入row的方式\n",
    "df=pd.DataFrame(columns=headcolumnlist)\n",
    "# df\n",
    "\n",
    "# import numpy as np \n",
    "# columnlists=np.zeros((10,10))\n",
    "# for i in range(10):\n",
    "#     print(i)\n",
    "#     columnlists.append(0)\n",
    "\n",
    "# columnlists = [[] for i in range(len(headcolumnlist))]\n",
    "\n",
    "# columnlists=columnlists.tolist\n",
    "\n",
    "# print(columnlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting...page 0 is snapping\n",
      "waiting...page 1 is snapping\n",
      "waiting...page 2 is snapping\n",
      "waiting...page 3 is snapping\n",
      "waiting...page 4 is snapping\n",
      "waiting...page 5 is snapping\n",
      "waiting...page 6 is snapping\n",
      "waiting...page 7 is snapping\n",
      "waiting...page 8 is snapping\n",
      "waiting...page 9 is snapping\n",
      "waiting...page 10 is snapping\n",
      "waiting...page 11 is snapping\n",
      "waiting...page 12 is snapping\n",
      "waiting...page 13 is snapping\n",
      "waiting...page 14 is snapping\n",
      "waiting...page 15 is snapping\n",
      "waiting...page 16 is snapping\n",
      "waiting...page 17 is snapping\n",
      "waiting...page 18 is snapping\n",
      "waiting...page 19 is snapping\n",
      "waiting...page 20 is snapping\n",
      "waiting...page 21 is snapping\n",
      "waiting...page 22 is snapping\n",
      "waiting...page 23 is snapping\n",
      "waiting...page 24 is snapping\n",
      "waiting...page 25 is snapping\n",
      "waiting...page 26 is snapping\n",
      "waiting...page 27 is snapping\n",
      "waiting...page 28 is snapping\n",
      "waiting...page 29 is snapping\n",
      "waiting...page 30 is snapping\n",
      "waiting...page 31 is snapping\n",
      "waiting...page 32 is snapping\n",
      "waiting...page 33 is snapping\n",
      "waiting...page 34 is snapping\n",
      "waiting...page 35 is snapping\n",
      "waiting...page 36 is snapping\n",
      "waiting...page 37 is snapping\n",
      "waiting...page 38 is snapping\n",
      "waiting...page 39 is snapping\n",
      "waiting...page 40 is snapping\n",
      "waiting...page 41 is snapping\n",
      "waiting...page 42 is snapping\n",
      "waiting...page 43 is snapping\n",
      "waiting...page 44 is snapping\n",
      "waiting...page 45 is snapping\n",
      "waiting...page 46 is snapping\n",
      "waiting...page 47 is snapping\n",
      "waiting...page 48 is snapping\n",
      "waiting...page 49 is snapping\n",
      "waiting...page 50 is snapping\n",
      "waiting...page 51 is snapping\n",
      "waiting...page 52 is snapping\n",
      "waiting...page 53 is snapping\n",
      "waiting...page 54 is snapping\n",
      "waiting...page 55 is snapping\n",
      "waiting...page 56 is snapping\n",
      "waiting...page 57 is snapping\n",
      "waiting...page 58 is snapping\n",
      "waiting...page 59 is snapping\n",
      "waiting...page 60 is snapping\n",
      "waiting...page 61 is snapping\n",
      "waiting...page 62 is snapping\n",
      "waiting...page 63 is snapping\n",
      "waiting...page 64 is snapping\n",
      "waiting...page 65 is snapping\n",
      "waiting...page 66 is snapping\n",
      "waiting...page 67 is snapping\n",
      "waiting...page 68 is snapping\n",
      "waiting...page 69 is snapping\n",
      "waiting...page 70 is snapping\n",
      "waiting...page 71 is snapping\n",
      "waiting...page 72 is snapping\n",
      "waiting...page 73 is snapping\n",
      "waiting...page 74 is snapping\n",
      "waiting...page 75 is snapping\n",
      "waiting...page 76 is snapping\n",
      "waiting...page 77 is snapping\n",
      "waiting...page 78 is snapping\n",
      "waiting...page 79 is snapping\n",
      "waiting...page 80 is snapping\n",
      "waiting...page 81 is snapping\n",
      "waiting...page 82 is snapping\n",
      "waiting...page 83 is snapping\n",
      "waiting...page 84 is snapping\n",
      "waiting...page 85 is snapping\n",
      "waiting...page 86 is snapping\n",
      "waiting...page 87 is snapping\n",
      "waiting...page 88 is snapping\n",
      "waiting...page 89 is snapping\n",
      "waiting...page 90 is snapping\n",
      "waiting...page 91 is snapping\n",
      "waiting...page 92 is snapping\n",
      "waiting...page 93 is snapping\n",
      "waiting...page 94 is snapping\n",
      "waiting...page 95 is snapping\n",
      "waiting...page 96 is snapping\n",
      "waiting...page 97 is snapping\n",
      "waiting...page 98 is snapping\n",
      "waiting...page 99 is snapping\n",
      "waiting...page 100 is snapping\n",
      "waiting...page 101 is snapping\n",
      "waiting...page 102 is snapping\n",
      "waiting...page 103 is snapping\n",
      "waiting...page 104 is snapping\n",
      "waiting...page 105 is snapping\n",
      "waiting...page 106 is snapping\n",
      "waiting...page 107 is snapping\n",
      "waiting...page 108 is snapping\n",
      "waiting...page 109 is snapping\n",
      "waiting...page 110 is snapping\n",
      "waiting...page 111 is snapping\n",
      "waiting...page 112 is snapping\n",
      "waiting...page 113 is snapping\n",
      "waiting...page 114 is snapping\n",
      "waiting...page 115 is snapping\n",
      "waiting...page 116 is snapping\n",
      "waiting...page 117 is snapping\n",
      "waiting...page 118 is snapping\n",
      "waiting...page 119 is snapping\n",
      "waiting...page 120 is snapping\n",
      "waiting...page 121 is snapping\n",
      "waiting...page 122 is snapping\n",
      "waiting...page 123 is snapping\n",
      "waiting...page 124 is snapping\n",
      "waiting...page 125 is snapping\n",
      "waiting...page 126 is snapping\n",
      "waiting...page 127 is snapping\n",
      "waiting...page 128 is snapping\n",
      "waiting...page 129 is snapping\n",
      "waiting...page 130 is snapping\n",
      "waiting...page 131 is snapping\n",
      "waiting...page 132 is snapping\n",
      "waiting...page 133 is snapping\n",
      "waiting...page 134 is snapping\n",
      "waiting...page 135 is snapping\n",
      "waiting...page 136 is snapping\n",
      "waiting...page 137 is snapping\n",
      "waiting...page 138 is snapping\n",
      "waiting...page 139 is snapping\n",
      "waiting...page 140 is snapping\n",
      "waiting...page 141 is snapping\n",
      "waiting...page 142 is snapping\n",
      "waiting...page 143 is snapping\n",
      "waiting...page 144 is snapping\n",
      "waiting...page 145 is snapping\n",
      "waiting...page 146 is snapping\n",
      "waiting...page 147 is snapping\n",
      "waiting...page 148 is snapping\n",
      "waiting...page 149 is snapping\n",
      "waiting...page 150 is snapping\n",
      "waiting...page 151 is snapping\n",
      "waiting...page 152 is snapping\n",
      "waiting...page 153 is snapping\n",
      "waiting...page 154 is snapping\n",
      "waiting...page 155 is snapping\n",
      "waiting...page 156 is snapping\n",
      "waiting...page 157 is snapping\n",
      "waiting...page 158 is snapping\n",
      "waiting...page 159 is snapping\n",
      "waiting...page 160 is snapping\n",
      "waiting...page 161 is snapping\n",
      "waiting...page 162 is snapping\n",
      "waiting...page 163 is snapping\n",
      "waiting...page 164 is snapping\n",
      "waiting...page 165 is snapping\n",
      "waiting...page 166 is snapping\n",
      "waiting...page 167 is snapping\n",
      "waiting...page 168 is snapping\n",
      "waiting...page 169 is snapping\n",
      "waiting...page 170 is snapping\n",
      "waiting...page 171 is snapping\n",
      "waiting...page 172 is snapping\n",
      "waiting...page 173 is snapping\n",
      "waiting...page 174 is snapping\n",
      "waiting...page 175 is snapping\n",
      "waiting...page 176 is snapping\n",
      "waiting...page 177 is snapping\n",
      "waiting...page 178 is snapping\n",
      "waiting...page 179 is snapping\n",
      "waiting...page 180 is snapping\n",
      "waiting...page 181 is snapping\n",
      "waiting...page 182 is snapping\n",
      "waiting...page 183 is snapping\n",
      "waiting...page 184 is snapping\n",
      "waiting...page 185 is snapping\n",
      "waiting...page 186 is snapping\n",
      "waiting...page 187 is snapping\n",
      "waiting...page 188 is snapping\n",
      "waiting...page 189 is snapping\n",
      "waiting...page 190 is snapping\n",
      "waiting...page 191 is snapping\n",
      "waiting...page 192 is snapping\n",
      "waiting...page 193 is snapping\n",
      "waiting...page 194 is snapping\n",
      "waiting...page 195 is snapping\n",
      "waiting...page 196 is snapping\n",
      "waiting...page 197 is snapping\n",
      "waiting...page 198 is snapping\n",
      "waiting...page 199 is snapping\n",
      "waiting...page 200 is snapping\n",
      "waiting...page 201 is snapping\n",
      "waiting...page 202 is snapping\n",
      "waiting...page 203 is snapping\n",
      "waiting...page 204 is snapping\n",
      "waiting...page 205 is snapping\n",
      "waiting...page 206 is snapping\n",
      "waiting...page 207 is snapping\n",
      "waiting...page 208 is snapping\n",
      "waiting...page 209 is snapping\n",
      "waiting...page 210 is snapping\n",
      "waiting...page 211 is snapping\n",
      "waiting...page 212 is snapping\n",
      "waiting...page 213 is snapping\n",
      "waiting...page 214 is snapping\n",
      "waiting...page 215 is snapping\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############循环点击totalpages-1次\n",
    "# for each in totalpages-1:\n",
    "\n",
    "\n",
    "# column1list=[]\n",
    "# column2list=[]\n",
    "# column3list=[]\n",
    "# column4list=[]\n",
    "# column5list=[]\n",
    "# column6list=[]\n",
    "# column7list=[]\n",
    "# column8list=[]\n",
    "# column9list=[]\n",
    "# column10list=[]\n",
    "\n",
    "# companylist=[]\n",
    "\n",
    "# 每行的数据作为新增加入DataFrame\n",
    "dict1={}\n",
    "\n",
    "for _ in range(0,totalpages):\n",
    "    \n",
    "    #########再一次抓取网页\n",
    "    ht = driver.page_source\n",
    "    ht=ht.encode('gb18030').decode('gb18030')\n",
    "    # ht=ht.encode(\"raw_unicode_escape\")\n",
    "\n",
    "    #print(ht)\n",
    "    # print(ht.)\n",
    "\n",
    "    # print(ht.encode('utf-8'))\n",
    "    # print(ht.encode('gb18030').encode(\"raw_unicode_escape\").decode(\"utf-8\"))\n",
    "\n",
    "    soup =BeautifulSoup(ht, 'lxml')\n",
    "#     print(soup)\n",
    "    \n",
    "    ##########找到各个元素\n",
    "    trs=soup.find('table', id=\"contentTable\").find('tbody').find_all('tr')\n",
    "    for each in trs: #每行\n",
    "        \n",
    "#         print(each)\n",
    "        for i in range(len(headcolumnlist)):\n",
    "#             print(i)\n",
    "#             columnlists[i].append(each.find_all('td')[i].text) #读取每行中每列的数据\n",
    "#             rowlist.append(each.find_all('td')[i].text)\n",
    "            dict1[headcolumnlist[i]]=each.find_all('td')[i].text\n",
    "#         columnlists[1].append(each.find_all('td')[0].text)\n",
    "            \n",
    "        # 重要！！先创建一个DataFrame，用来增加进数据框的最后一行\n",
    "        \n",
    "#         newrow=pd.DataFrame(rowlist)  # 自定义索引为：1 ，这里也可以不设置index\n",
    "#         print(dict1)\n",
    "        df=df.append(dict1,ignore_index=True)\n",
    "#         column1value=each.find_all('td')[0].text\n",
    "#         column2value=each.find_all('td')[1].text\n",
    "#         column3value=each.find_all('td')[2].text\n",
    "#         column4value=each.find_all('td')[3].text\n",
    "#         column5value=each.find_all('td')[4].text\n",
    "#         column6value=each.find_all('td')[5].text\n",
    "#         column7value=each.find_all('td')[6].text\n",
    "#         column8value=each.find_all('td')[7].text\n",
    "#         column9value=each.find_all('td')[8].text\n",
    "#         column10value=each.find_all('td')[9].text\n",
    "        \n",
    "#         column1list.append(each.find_all('td')[0].text)\n",
    "#         column2list.append(each.find_all('td')[1].text)\n",
    "#         column3list.append(each.find_all('td')[2].text)\n",
    "#         column4list.append(each.find_all('td')[3].text)\n",
    "#         column5list.append(each.find_all('td')[4].text)\n",
    "#         column6list.append(each.find_all('td')[5].text)\n",
    "#         column7list.append(each.find_all('td')[6].text)\n",
    "#         column8list.append(each.find_all('td')[7].text)\n",
    "#         column9list.append(each.find_all('td')[8].text)\n",
    "#         column10list.append(each.find_all('td')[9].text)\n",
    "        \n",
    "#         companylist.append(each.find_all('a')[1].text)\n",
    "#         df.append()\n",
    "    \n",
    "#         print(each.find_all('a')[1].text)\n",
    "#         print('\\n')\n",
    "    \n",
    "    #########\n",
    "    # pagenext=driver.find_element_by_class_name(\"page-next\")\n",
    "\n",
    "    # inputbox=driver.find_element_by_id(\"search-input\")\n",
    "\n",
    "    # pagenextxpath=driver.find_element_by_xpath(\"/html/body/div/div[0]/div\")\n",
    "    #######################################################\n",
    "    #找到下一页的位置，注意要到a href这个地方，不是上一级\n",
    "    pagenextxpath=driver.find_element_by_xpath(\"//li[@class='page-next']/a\")\n",
    "    # print(pagenextxpath.text)\n",
    "    \n",
    "    \n",
    "    #######################################################\n",
    "    #鼠标移动找到下一页的位置，然后点击\n",
    "    action=ActionChains(driver).move_to_element(pagenextxpath)\n",
    "    action.click().perform() \n",
    "    time.sleep(2)\n",
    "    print(\"waiting...page {} is snapping\".format(_)  )\n",
    "    \n",
    "print('Done.')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>公告日期</th>\n",
       "      <th>证券代码</th>\n",
       "      <th>证券简称</th>\n",
       "      <th>申万二级行业</th>\n",
       "      <th>报告年度</th>\n",
       "      <th>业绩类型</th>\n",
       "      <th>业绩预告内容</th>\n",
       "      <th>业绩变化原因</th>\n",
       "      <th>净利润增长幅上限(%)</th>\n",
       "      <th>净利润增长幅下限(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>600053</td>\n",
       "      <td>九鼎投资</td>\n",
       "      <td>其他金融业</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>业绩大幅上升</td>\n",
       "      <td>预计公司2019年01-12月归属于上市公司股东的净利润与上年同期相比增长150%-190%。</td>\n",
       "      <td>1、房地产业务的影响2019年房地产销售实现收入为15.79亿元，同比增长1494.95%；...</td>\n",
       "      <td>190</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>600127</td>\n",
       "      <td>金健米业</td>\n",
       "      <td>农副食品加工业</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>预计扭亏</td>\n",
       "      <td>预计公司2019年01-12月归属于上市公司股东的净利润为1000万元-1500万元，与上年...</td>\n",
       "      <td>1.公司全力拓宽营销渠道，对接新零售，本期营业收入同比有较大幅度增加，毛利额同比相应增加。2...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>600238</td>\n",
       "      <td>ST椰岛</td>\n",
       "      <td>酒、饮料和精制茶制造业</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>业绩预亏</td>\n",
       "      <td>预计公司2019年01-12月归属于上市公司股东的净利润亏损24000万元-28800万元。</td>\n",
       "      <td>1、本报告期内，由于市场竞争加剧，公司酒类业务产品市场销售加剧下滑，新产品市场推广未达预期。...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         公告日期    证券代码  证券简称       申万二级行业        报告年度    业绩类型  \\\n",
       "0  2020-01-31  600053  九鼎投资        其他金融业  2019-12-31  业绩大幅上升   \n",
       "1  2020-01-31  600127  金健米业      农副食品加工业  2019-12-31    预计扭亏   \n",
       "2  2020-01-31  600238  ST椰岛  酒、饮料和精制茶制造业  2019-12-31    业绩预亏   \n",
       "\n",
       "                                              业绩预告内容  \\\n",
       "0    预计公司2019年01-12月归属于上市公司股东的净利润与上年同期相比增长150%-190%。   \n",
       "1  预计公司2019年01-12月归属于上市公司股东的净利润为1000万元-1500万元，与上年...   \n",
       "2     预计公司2019年01-12月归属于上市公司股东的净利润亏损24000万元-28800万元。   \n",
       "\n",
       "                                              业绩变化原因 净利润增长幅上限(%) 净利润增长幅下限(%)  \n",
       "0  1、房地产业务的影响2019年房地产销售实现收入为15.79亿元，同比增长1494.95%；...         190         150  \n",
       "1  1.公司全力拓宽营销渠道，对接新零售，本期营业收入同比有较大幅度增加，毛利额同比相应增加。2...           -           -  \n",
       "2  1、本报告期内，由于市场竞争加剧，公司酒类业务产品市场销售加剧下滑，新产品市场推广未达预期。...           -           -  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            公告日期    证券代码  证券简称            申万二级行业        报告年度    业绩类型  \\\n",
      "285   2020-01-23  300033   同花顺        软件和信息技术服务业  2019-12-31  业绩大幅上升   \n",
      "663   2020-01-22  300059  东方财富            资本市场服务  2019-12-31  业绩大幅上升   \n",
      "1276  2020-01-20  600570  恒生电子        软件和信息技术服务业  2019-12-31  业绩大幅上升   \n",
      "1563  2020-01-16  000157  中联重科           专用设备制造业  2019-12-31  业绩大幅上升   \n",
      "1797  2020-01-09  300327  中颖电子  计算机、通信和其他电子设备制造业  2019-12-31    业绩预增   \n",
      "1868  2020-01-02  600519  贵州茅台       酒、饮料和精制茶制造业  2019-12-31    业绩预增   \n",
      "1928  2019-10-31  002027  分众传媒             商务服务业  2019-12-31  业绩大幅下降   \n",
      "\n",
      "                                                 业绩预告内容  \\\n",
      "285   预计公司2019年01-12月归属于上市公司股东的净利润为85581.13万元-104599...   \n",
      "663   预计公司2019年01-12月归属于上市公司股东的净利润为172000万元-192000万元...   \n",
      "1276  预计公司2019年01-12月归属于上市公司股东的净利润为129074万元-134237万元...   \n",
      "1563  预计公司2019年01-12月归属于上市公司股东的净利润为430000万元-450000万元...   \n",
      "1797  预计公司2019年01-12月归属于母公司股东的净利润为18512万元-19353万元，与上...   \n",
      "1868  预计公司2019年01-12月归属于上市公司股东的净利润为4050000万元，与上年同期相比...   \n",
      "1928  预计公司2019年01-12月归属于上市公司股东的净利润为175034.75万元-20503...   \n",
      "\n",
      "                                                 业绩变化原因 净利润增长幅上限(%)  \\\n",
      "285   2019年,随着国内A股市场活跃度回升,投资者对证券金融资讯的需求有所增加,销售收款相应增长...          65   \n",
      "663   2019年度，资本市场景气度活跃，股票交易额同比大幅增加，公司证券业务实现快速发展，证券业务...      100.27   \n",
      "1276  （1）报告期内执行新金融工具会计准则，公司持有的金融资产产生的公允价值变动收益大幅增加、处置...         108   \n",
      "1563  1、2019年国内房地产和基建等下游行业需求保持较好增速，工程机械行业持续景气，全年保持中高...      122.79   \n",
      "1797  公司全年销售同比增长，带动盈利同向增长。2019年度非经常性损益对归属于公司净利润的影响金额...          15   \n",
      "1868                                                未披露          15   \n",
      "1928  2019年至今，中国广告市场受宏观经济影响需求疲软，叠加公司自身客户结构调整的影响，致使公司...       69.94   \n",
      "\n",
      "     净利润增长幅下限(%)  \n",
      "285           35  \n",
      "663        79.41  \n",
      "1276         100  \n",
      "1563      112.89  \n",
      "1797          10  \n",
      "1868          15  \n",
      "1928       64.79  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#和自选股中的股票进行比较，显示出来哪些有股票业绩披露\n",
    "#dataframes的批量筛选\n",
    "\n",
    "#找出自选股\n",
    "codelist=[]\n",
    "with open(selectedcodepath, newline='', encoding = 'UTF-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        codelist.append(row['code'])\n",
    "#         print(row)\n",
    "#         print(row['code'])\n",
    "\n",
    "\n",
    "# df_selected=df[~df['证券代码'].isin(codelist)]\n",
    "df_selected=df[df['证券代码'].isin(codelist)]\n",
    "print(df_selected)\n",
    "# csv_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df=pd.DataFrame(columnlists,columns=headcolumnlist) #从二维list创建DataFrame\n",
    "\n",
    "# for i in range(len(columnlists)):\n",
    "#     df=df.insert(i,columnlists[i])\n",
    "\n",
    "# df=pd.DataFrame(list(zip(column1list, column2list)))\n",
    "# df=pd.DataFrame(columnlists,columns=headcolumnlist)\n",
    "# df\n",
    "# df[0]=column1list\n",
    "\n",
    "filewholepath=downloadpath+downloadfilename\n",
    "# df.to_csv(filewholepath, encoding='utf-8-sig')\n",
    "df.to_excel(filewholepath, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(pagenext.location)\n",
    "# print(inputbox.location)\n",
    "# print(pagenextxpath.location)\n",
    "# print(companylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df=pd.DataFrame(column10list,columns=headcolumnlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# action.context_click().perform() \n",
    "# action.send_keys(Keys.ARROW_DOWN) \n",
    "\n",
    "# ActionChains(driver).move(1592, 756).click().perform() \n",
    "# ActionChains(driver).move(1592, 756).context_click().perform()\n",
    "# time.sleep(3) \n",
    "\n",
    "# action2=ActionChains(driver).move_to_element(inputbox)\n",
    "# action2.send_keys(Keys.ARROW_DOWN) \n",
    "# action2.send_keys(\"300383\") \n",
    "# action2.send_keys(Keys.RETURN) \n",
    "# time.sleep(3) \n",
    "\n",
    "########################################\n",
    "#在搜索框中搜索字符串\n",
    "# inputbox.clear\n",
    "# inputbox.send_keys(Keys.ARROW_DOWN) \n",
    "# inputbox.send_keys(\"300383\") \n",
    "# inputbox.send_keys(Keys.RETURN)\n",
    "#######################################\n",
    "\n",
    "# action.context_click(pagenext)\n",
    "# pagenext.send_keys(Keys.RETURN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'I:\\\\MyMobileBooks_800_Reading\\\\MyTSSnippets/DataRepository/db00111/2019Q4.csv' does not exist: b'I:\\\\MyMobileBooks_800_Reading\\\\MyTSSnippets/DataRepository/db00111/2019Q4.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c8d14a5aedce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# print(conbondspath)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mconbondsdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconbondspath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8-sig'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;31m# print(conbondsdf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'I:\\\\MyMobileBooks_800_Reading\\\\MyTSSnippets/DataRepository/db00111/2019Q4.csv' does not exist: b'I:\\\\MyMobileBooks_800_Reading\\\\MyTSSnippets/DataRepository/db00111/2019Q4.csv'"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#2.dataframe比较\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "#当前文件的路径\n",
    "pwd = os.getcwd()\n",
    "# print(pwd)\n",
    "#当前文件的父路径\n",
    "father_path=os.path.abspath(os.path.dirname(pwd)+os.path.sep+\".\")\n",
    "# print(father_path)\n",
    "\n",
    "selectedcodepath=father_path + \"/DataRepository/db00021/stocklist1.csv\"\n",
    "# print(selectedcodepath)\n",
    "conbondspath=father_path + \"/DataRepository/db00111/2019Q4.csv\"\n",
    "# print(conbondspath)\n",
    "\n",
    "conbondsdf=pd.read_csv(conbondspath,encoding='utf-8-sig')\n",
    "# print(conbondsdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#找出自选股\n",
    "codelist=[]\n",
    "with open(selectedcodepath, newline='', encoding = 'UTF-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        codelist.append(row['code'])\n",
    "#         print(row)\n",
    "#         print(row['code'])\n",
    "# print(codelist)\n",
    "\n",
    "# print(conbondsdf['code'])\n",
    "conbondsdf=conbondsdf[conbondsdf['证券代码'].isin(codelist)]\n",
    "print(conbondsdf)\n",
    "# ?conbondsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conbondsdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5816b1b5fe74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;31m#     print(str(row))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconbondsdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[1;31m#   print(index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;31m#   print(str(row))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conbondsdf' is not defined"
     ]
    }
   ],
   "source": [
    "# print(conbondsdf.values.tolist())\n",
    "# for each in conbondsdf:\n",
    "#     print(each.values.tolist())\n",
    "\n",
    "#这里是tuple\n",
    "# for row in conbondsdf.iterrows(): \n",
    "# for row in conbondsdf: \n",
    "#     print(list(row))\n",
    "#     print(row.head)\n",
    "#     mailsubject=row['证券代码']\n",
    "#     mailcontent=row.tolist()\n",
    "#     print(str(row))\n",
    "\n",
    "for index,row in conbondsdf.iterrows():\n",
    "#   print(index)\n",
    "#   print(str(row))\n",
    "    mailsubject=row['证券简称']\n",
    "    mailcontent=','.join(str(i) for i in row)\n",
    "#     print(mailcontent)\n",
    "\n",
    "# mailcontent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.header import Header\n",
    "\n",
    "mail=smtplib.SMTP('smtp.office365.com',587)\n",
    "mail.ehlo()\n",
    "mail.starttls()\n",
    "\n",
    "mail.login('aaronyinyong@hotmail.com','yinrong090114')\n",
    "message = MIMEText(mailcontent, 'plain', 'utf-8')   #发送的内容\n",
    "message['From'] = Header(\"aaronyinyong@hotmail.com\", 'utf-8')   #发件人\n",
    "# message['To'] =\"todo@mail.dida365.com\"   #收件人\n",
    "message['To'] =\"aaronyinyong@hotmail.com\"   #收件人\n",
    "message['Subject'] = Header(str(mailsubject), 'utf-8')  #邮件标题\n",
    "mail.sendmail('aaronyinyong@hotmail.com','todo@mail.dida365.com',message.as_string())\n",
    "mail.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
